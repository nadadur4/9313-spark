i = 23
(i // 10) * 10
i // 10
i - i // 10
i - i % 10
i % 10
229 - 29 % 10
29 - 29 % 10
-54 - (-54) % 10
-9 - (-9) % 10
9 - 9 % 10
-9 % 10
-8 % 10
-8 -2
abs(-3)
abs(i) - abs(i) % 10
abs(-8) - abs(-8)  % 10
q()
quit()
23 // 10 * 10
-3 // 10 * 10
-3 - (-3) % 10
28 - 28 % 10
quit()
with file('sales.csv', 'r') as sales:
for line in sales.readlines():
with file('sales.csv', 'r') as sales:
	for line in sales.readlines()
with file('sales.csv', 'r') as sales:
	for line in sales.readlines():
		if line.strip().split()[2] < -1:
			print(line)
help(file)
help(f)
help(open)
with open('sales.csv', 'r') as sales:
	for line in sales.readlines():
		if line.strip().split()[2] < -1:
			print(line)
quit()
import numpy
import pandas
quit()
import mrjob
quit()
3 / 0
3 / None
quit()
None == 0
None == 2
None == NaN
None == \0
\0
0 == False
quit()
help(getattr)
quit()
ls
print(1)
quit()
foo = 1
plusFoo = lambda x: x + foo
plusFoo(5)
foo = 10
plusFoo(5)
exit
python
" a '
"a 'b' "
set
set(2,3, 4, 3, 2, 3)
set([2, 3, 4, 3, 3, 2, 5])
{2, 3, 4, 5}
{2, 3, 3, 2, 3, 4, 5, 2}
q()
quit()
l = [2, 3, 4, 5]
l
set(word for word in l)
quit()
from pyspark.sql import functions as F
df_raw = spark.createDataFrame(
    ["20030219,first headline",
     "20040501,second headline"], "string")
from pyspark.sql.functions import *
from pyspark.sql.session import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import sys
rawText = spark.read.text(abcnews.txt)
rawText = spark.read.text(file://abcnews.txt)
rawText = spark.read.text(file:///abcnews.txt)
rawText = spark.read.text("abcnews.txt")
rawText = spark.read.text("file:///home/nadserver/spark-3.5.6-bin-hadoop3/ass2/abcnews.txt")
date, headline = split(rawText)
date, headline = split(rawText, ',')
date, headline = split(rawText.value, ',')
parts = split(rawText.value, ',')
parts
parts.collect()
parts[0]
parts[0].show()
parts.show()
df = (df_raw
      .withColumn("date",   parts.getItem(0))       # or parts[0]
      .withColumn("text",   parts.getItem(1))       # or parts[1]
      .withColumn("year",   F.substring(parts[0], 1, 4))
      .select("year", "text"))
df = (df_raw
      .withColumn("date",   parts.getItem(0))       # or parts[0]
      .withColumn("text",   parts.getItem(1))       # or parts[1]
      .withColumn("year",   F.substring(parts[0], 1, 4))
df = (rawText
.withColumn("year", substring(parts.getItem(0), 1, 4))
      .withColumn("tokens",
                  array_distinct(                              # one copy per headline
                      split(lower(parts.getItem(1)), " ")))    # lowercase + split on space
      .select("year", "tokens"))
df.show(truncate=False)
help(type)
type(parts[0])
parts[0]
parts[0].show()
df
df.show()
parts
parts[0].substr(1, 4)
parts[0].substr(1, 4).show()
print(parts[0].substr(1, 4))
yearCol = substring(parts[0], 1, 4)
        headlineCol = parts[1]
        initDF = (rawText
                  .withColumn("year", yearCol)
                  .withColumn("headline", headlineCol)
                  .drop("value")
headlineCol = parts[1]
initDF = (rawText
                  .withColumn("year", yearCol)
                  .withColumn("headline", headlineCol)
                  .drop("value")
        )
initDF.show()
initDF.show(truncated=False)
initDF.show(truncate=False)
df.show(truncate=False)
headlineCol = split(lower(parts[1]), " ")
initDF = (rawText
                  .withColumn("year", yearCol)
                  .withColumn("headline", headlineCol)
                  .drop("value")
        )
initDF.show(truncate=False)
initDF.select("year").alias("YYYY").show()
initDF.select("year").alias("YYYY").collec()
initDF.select("year").alias("YYYY").collect()
initDF.select("year").alias("YYYY").collect().show()
rawWordFreqs = (initDF
                        .select(explode(col("headline"))).alias("word")
                        .groupBy("word")
                        .count()
        )
rawWordFreqs = (initDF
                        .select(explode(col("headline")).alias("word"))
                        .groupBy("word")
                        .count()
        )
rawWordFreqs.show()
rawWordFreqs.orderBy("word")
rawWordFreqs.orderBy("count")
rawWordFreqs.orderBy("count").show()
rawWordFreqs.orderBy(["count", "word"], ascending=[False, True]).show()
stopwordsList = (rawWordFreqs
                         .orderBy(["count", "word"], ascending=[False, True])
                         .limit(n_stopwords)
                         .select("word")
        )
stopwordsList = (rawWordFreqs
                         .orderBy(["count", "word"], ascending=[False, True])
.limit(1)
.select("word")
)
stopwordsList.show()
explode(stopwordsList).collect()
explode(stopwordsList)
explode(stopwordsList.word).collect()
explode(stopwordsList.word)
explode(stopwordsList.word).show()
stopwordsList.show()
stopwordsList.collect9)
stopwordsList.collect()
stopwordsList.rdd.flatMap(lambda x: x).collect()
stopwordsList.show()
stopwordsList.distinct()
stopwordsList.distinct().show()
stopwordsList.collect()
type(stopwrodsList.collect())
type(stopwordsList.collect())
print(stopwrodsList.collect())
print(stopwordsList.collect())
set(stopwordsList.collect())
set(stopwordsList.rdd.flatMap(lambda x: x).collect())
stopwordsDF = (rawWordFreqs
                         .orderBy(["count", "word"], ascending=[False, True])
.limit(2)
.select("word")
)
stopwordsDF.collect()
stopwordsDF.agg({"word": "collect_set"}).show()
stopwordsDF.agg({"word": "collect_set"}).alias("stopwords").show()
stopwordsDF.agg({"word": "collect_set"}).select("collect_set(word").alias("stopwords").show()
stopwordsDF.agg({"word": "collect_set"}).select("collect_set(word)").alias("stopwords").show()
stopwordsDF.agg({"word": "collect_set"}).show()
array_distinct(stopwordsDF.agg({"word": "collect_set"}))
stopwordsDF.agg({"word": "collect_set"}).show()
stopwordsDF.agg({"word": "collect_set"}).alias("stopwords").show()
stopwordsDF.agg({"word": "collect_set"}).alias("stopwords").first().show()
stopwordsDF.agg({"word": "collect_set"}).alias("stopwords").first()
stopwordsDF.agg({"word": "collect_set"}).alias("stopwords").first()[1]
stopwordsDF.agg({"word": "collect_set"}).alias("stopwords").first()[0]
stopwordsDF.agg({"word": "collect_set"}).alias("stopwords")[0][0]
stopwordsDF.agg({"word": "collect_set"}).alias("stopwords").collect()[0]
stopwordsDF.agg({"word": "collect_set"}).alias("stopwords").collect()
stopwordsDF.agg({"word": "collect_set"}).alias("stopwords").collect()[0][0]
type(stopwordsDF.agg({"word": "collect_set"}).alias("stopwords").collect()[0][0])
[2, 3, 4]
type([2, 3, 4])
stopwordsDF.agg({"word": "collect_set"}).alias("stopwords").collect()[0][0].isinstance([2, 3, 4])
isinstance(stopwordsDF.agg({"word": "collect_set"}).alias("stopwords").collect()[0][0], [2, 3, 4])
isinstance(type(stopwordsDF.agg({"word": "collect_set"}).alias("stopwords").collect()[0][0]), type([2, 3, 4]))
isinstance(type(2), type(3))
?isinstance
help(isintance)
help(isinstance)
spark.range(3)
spark.range(3).show()
df = spark.createDataFrame([1, 2, 3, 3, 4], "int")
df.join(df_b, df.value == spark.range(3).id).show()
df.join(df, df.value == spark.range(3).id).show()
range3 = spark.range(3)
df.join(range3, df.value == range3.id).show()
stopwordsDF.collect()
stopwordsDF.collect()[0]
stopwordsDF.show()[0]
stopwordsDF.show()
initDF.select("year", explode(col("headline")).alias("headline_word"))
yearWordDF = initDF.select("year", explode(col("headline")).alias("headline_word"))
yearWordDF.show()
yearWordDF.show(truncate=False) | less
yearWordDF.show(truncate=False)
yearWordDF.show()
yearWordDF = initDF.distinct("year", explode(col("headline")).alias("headline_word"))
yearWordDF = initDF.select("year", explode(col("headline")).alias("headline_word"))
headlinesDF = initDF
hlExploded = (headlinesDF
                      .select("year", explode(col("headline")).alias("word"))
        )
hlFiltered = (hlExploded
                      .join(broadcast(stopwordsDF), "word", "left")
                      .filter(col("stopwordsDF.word").isNull())
        )
hlFiltered = (hlExploded
                      .join(broadcast(stopwordsDF), "word", "left")
                      .filter(col("word").isNull())
        )
hlFiltered.show()
stopwordsDF = (rawWordFreqs
                       .orderBy(["count", "word"], ascending=[False, True])
                       .limit(n_stopwords)
                       .select(col("word").alias("stopword"))
        )
        hlExploded = (headlinesDF
                      .select("year", explode(col("headline")).alias("word"))
        )
        hlFiltered = (hlExploded
                      .join(broadcast(stopwordsDF),
                            hlExploded.word == stopwordsDF.stopword,
                            "left"
                      )
                      .filter(col("stopword").isNull())
stopwordsDF = (rawWordFreqs
                .orderBy(["count", "word"], ascending=[False, True])
                .limit(n_stopwords)
                .select(col("word").alias("stopword"))
)
hlExploded = (headlinesDF
                .select("year", explode(col("headline")).alias("word"))
)
hlFiltered = (hlExploded
                .join(broadcast(stopwordsDF),
                    hlExploded.word == stopwordsDF.stopword,
                    "left"
                )
                .filter(col("stopword").isNull())
stopwordsDF = (rawWordFreqs
                       .orderBy(["count", "word"], ascending=[False, True])
.limit(1)
.select(col("word").alias("stopword"))
)
hlExploded = (headlinesDF
                .select("year", explode(col("headline")).alias("word"))
)
hlFiltered = (hlExploded
                .join(broadcast(stopwordsDF),
                    hlExploded.word == stopwordsDF.stopword,
                    "left"
                )
                .filter(col("stopword").isNull())
)
hlExploded
hlExploded.show()
hlFiltered.show()
hlFiltered = (hlExploded
                      .join(broadcast(stopwordsDF),
                            hlExploded.word == stopwordsDF.stopword,
                            "left"
                      )
)
hlFiltered.show()
hlFiltered = (hlExploded
                      .join(broadcast(stopwordsDF),
                            hlExploded.word == stopwordsDF.stopword,
                            "left"
                      )
                .filter(col("stopword").isNull())
)
hlFiltered.show()
hlFiltered = (hlExploded
                      .join(broadcast(stopwordsDF),
                            hlExploded.word == stopwordsDF.stopword,
                            "left"
                      )
                      .filter(col("stopword").isNull())
                      .groupBy("year")
        )
hlFiltered.show()
hlFiltered = (hlExploded
                      .join(broadcast(stopwordsDF),
                            hlExploded.word == stopwordsDF.stopword,
                            "left"
                      )
hlFiltered = (hlExploded
.join(stopwordsDF), hlExploded.word == stopwordsDF.stopword, "left")
.filter(col("stopword").isNull())
hlFiltered = (hlExploded
.join(stopwordsDF, hlExploded.word == stopwordsDF.stopword, "left")
.filter(col("stopword").isNull())
)
hlFiltered
hlFiltered.show()
hleFiltered = hlFiltered
hleDistinct = select_set(hleFiltered)
hleDistinct = collect_set(hleFiltered)
hleDistinct = hleFiltered.withColumn(collect_set(col("word")))
hleDistinct = hleFiltered.withColumn(col(collect_set(col("word"))))
hleDistinct = hleFiltered.withColumn(col(collect_set(hleFiltered.word)))
hleDistinct = hleFiltered.withColumn(collect_set(hleFiltered.word))
hleDistinct = hleFiltered.withColumn("colword", collect_set(hleFiltered.word))
hleDistinct = hleFiltered.withColumn("colword", hleFiltered.word.agg())
hleFiltered.show()
hleFiltered.select(col("word").distinct()))
hleFiltered.select(col("word").distinct())
hleFiltered.distinct(col("word"))
hleFiltered.distinct()
hleFiltered.distinct().show()
hleFiltered.distinct().select("year", "word").orderBy(["year", "word"])
hleFiltered.distinct().select("year", "word").orderBy(["year", "word"]).show()
hleFiltered.distinct().select("year", "word").orderBy(["year", "word"]).filter(col("word") == "council")
hleFiltered.distinct().select("year", "word").orderBy(["year", "word"]).filter(col("word") == "council").show()
hleFiltered.dropDuplicates()
hleFiltered.dropDuplicates().show()
hleFiltered.dropDuplicates().orderby(["year", "word"]).show()
hleFiltered.dropDuplicates().select("year", "word").orderBy(["year", "word"]).show()
hleFiltered.dropDuplicates().select("year", "word").orderBy(["year", "word"]).filter(col("word") == "council").show()
hleFiltered.dropDuplicates().select("year", "word").orderBy(["year", "word"]).filter(col("word") == "coronavirus").show()
headlinesDF.show()
headlinesDF.show(truncate=False)
hleFiltered/show()
hleFiltered.show()
hleFiltered.agg(sum())
hleFiltered.agg(count)
hleFiltered.agg(count("year", "word"))
hleFiltered.agg(count(("year", "word")))
hleFiltered.agg(count("year", "word"))
hleFiltered.agg(col("year)
hleFiltered.show()
quit()
nStopwords = int(stopwords)
        nTopics = int(topic)
        nTopPerYear = int(k)
        # We parse the text using various methods from pyspark.sql.functions
        # and arrange our new columns into an initial DataFrame
        rawText = spark.read.text(inputPath)
        parts = split(rawText.value, ',', 2)
        yearCol = substring(parts[0], 1, 4)
        headlineCol = array_distinct(split(lower(parts[1]), " "))
        headlinesDF = (rawText
                       .withColumn("year", yearCol)
                       .withColumn("headline", headlineCol)
                       .drop("value")
        )
        # alias() is like SQL as
        rawWordFreqs = (headlinesDF
                        .select(explode(col("headline")).alias("word"))
                        .groupBy("word")
                        .count()
        )
        stopwordsDF = (rawWordFreqs
                       .orderBy(["count", "word"], ascending=[False, True])
                       .limit(n_stopwords)
                       .select(col("word").alias("stopword"))
        )
        hlExploded = (headlinesDF
                      .select("year", explode(col("headline")).alias("word"))
        )
        # Left join every word in every headline with the list of stopwords.
        # All non-stopword rows will have NULL in the stopword field
        # New structure: [year | word] for every word in every headline
        hleFiltered = (hlExploded
                       .join(broadcast(stopwordsDF),
                             hlExploded.word == stopwordsDF.stopword,
                             "left"
                       )
                       .filter(col("stopword").isNull())
        )
from pyspark.sql.session import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import sys
from pyspark.sql.functions import concat_ws
help(concat_ws)
help(char)
quit()
chr(9)
f"a{chr(9)}b"
f"a\tb"
f"a \t b"
f"a \n b"
print("a\nb")
        
(aucid, bid, bidtime, bidder,
bidderrate, openbid, price,
itemtype, dtl) = range(1, 9)
itemtype, dtl) = range(9)
(aucid, bid, bidtime, bidder,
itemtype, dtl) = range(1, 9)
(aucid, bid, bidtime, bidder,
bidderrate, openbid, price,
itemtype, dtl) = range(9)
aucid
bid
:q
quit()
rawText = spark.read.csv(inputPath)
rawText = spark.read.csv(auctiondata.csv)
rawText = spark.read.csv("auctiondata.csv")
rawText.show()
quit()
def lineParse(line, prefix=''):
    recId, coOrd, terms = line.strip().split('#')
    numId = int(recId[1:])
    x, y = coOrd[1:-1].split(',')
    x, y = float(x), float(y)
    tokens = set(terms.split())
    parsedLine = (prefix, numId, recId, x, y, tokens)
    return parsedLine
def lineParse(line, prefix=''):
    recId, coOrd, terms = line.strip().split('#')
    numId = int(recId[1:])
    x, y = coOrd[1:-1].split(',')
    x, y = float(x), float(y)
    tokens = set(terms.split())
    parsedLine = (prefix, numId, recId, x, y, tokens)
    return parsedLine
rddA = sc.textFile("A.txt").map(lambda line: lineParse(line, 'A'))
rddB = sc.textFile("B.txt").map(lambda line: lineParse(line, 'B'))
rddA.collect()
rddB.collect()
tokenFreqsA = rddA.flatMap(lambda rec: [(term, 1) for term in rec[5]])
tokenFreqsB = rddB.flatMap(lambda rec: [(term, 1) for term in rec[5]])
tokenFreqs = tokenFreqsA.union(tokenFreqsB).reduceByKey(add)
from operator import add
tokenFreqs = tokenFreqsA.union(tokenFreqsB).reduceByKey(add)
tokenOrder = (tokenFreqs
              .sortBy(lambda pair: (pair[1], pair[0]))
              .map(lambda pair: pair[0])
              .zipWithIndex())
tokenOrder.collect()
print(type(tokenOrder.collect()[1][1])
)
print(type(tokenOrder.collect()[1][0]))
tokenFreqs
                      .sortBy(lambda pair: (pair[1], pair[0]))
tokenFreqs.sortBy(lambda pair: (pair[1], pair[0]))
tokenFreqs.sortBy(lambda pair: (pair[1], pair[0])).collect()
tokenOrder.collect()
termFreqsA.collect()
tokenFreqsA.collect()
tokenFreqs.collect9)
tokenFreqs.collect()
tokenFreqsA.reduceByKey(add)
tokenFreqsA.reduceByKey(add).collect()
tokenFreqsB.reduceByKey(add).collect()
tokenFreqs.collect()
tokenFreqs.sortBy(lambda pair: (pair[1], pair[0]))
tokenFreqs.sortBy(lambda pair: (pair[1], pair[0])).collect()
set(tokenFreqs.sortBy(lambda pair: (pair[1], pair[0])).collect())
tokenFreqs.sortBy(lambda pair: (pair[1], pair[0])).map(lambda pair: pair[0]).collect()
docFreqs = tokenFreqs.sortBy(lambda pair: (pair[1], pair[0])).map(lambda pair: pair[0]).collect()
docFreqs
sc.broadcast(docFreqs)
bcdf = sc.broadcast(docFreqs)
bcdf
bcdf.value
sc.parallelize(['a', 'b', 'c', 'd'])
bcdf.value
rddA
rddA.collect()
exit
exit()
import sys
from pyspark import SparkConf, SparkContext
from pyspark.sql.session import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from operator import add
DATASET, NUMID, RECID, X, Y, TERMS = 0, 1, 2, 3, 4, 5
def lineParse(line):
    recId, coOrd, terms = line.strip().split('#')
    dataset = recId[0]
    numId = int(recId[1:])
    x, y = coOrd.strip('()').split(',')
    terms = set(terms.split())
    parsedLine = (dataset, numId, recId, x, y, terms)
    return parsedLine
rddA = sc.textFile("in/A.in").map(lambda line: lineParse(line))
rddB = sc.textFile("in/B.in").map(lambda line: lineParse(line))
rddA.collect()
        rddA = sc.textFile(inputpathA).map(lineParse)
        rddB = sc.textFile(inputpathB).map(lineParse)
rddA = sc.textFile("in/A.in").map(lineParse)
rddB = sc.textFile("in/B.in").map(lineParse)
rddA.collect()
rddA.show()
rdd.collect().foreach(print)
rddA.collect().foreach(print)
for row in rddA.collect():
	print(row)
tokenFreqsA
termPairsA = rddA.flatMap(lambda rec: [(term, 1) for term in rec[TERMS]])
termPairsB = rddB.flatMap(lambda rec: [(term, 1) for term in rec[TERMS]])
docFreqs = termPairsA.union(termPairsB).reduceByKey(add)
termOrder = (docFreqs
	      .sortBy(lambda pair: (pair[1], pair[0]))
	      .map(lambda pair: pair[0])
	      .collectAsMap()
)
bcTermOrder = sc.broadcast(termOrder)
termOrder = (docFreqs
	      .sortBy(lambda pair: (pair[1], pair[0]))
	      .map(lambda pair: pair[0])
	      .zipWithIndex()
	      .collectAsMap()
)
bcTermOrder = sc.broadcast(termOrder)
bcTermOrder.value
def recAddPrefix(rec, bcTermOrder, s):
    dataset, numId, recId, x, y, terms = rec
    termOrder = bcTermOrder.value
    n = len(termOrder)
    ordered = sorted(terms, key=lambda term: (termOrder[term], term))
    l = int(math.ceil(n * s))
    prefix_len = n - l + 1
    prefix = tuple(ordered[:prefix_len])
    prefixRec = (dataset, numId, recId, x, y, terms, prefix)
    return prefixRec
        rddA = rddA.map(recAddPrefix)
        rddB = rddB.map(recAddPrefix)
rddA = rddA.map(recAddPrefix)
rddB = rddB.map(recAddPrefix)
for i in rddA.collect():
	print(i)
rddA.collect()
        rddA = rddA.map(lambda rec: recAddPrefix(rec, bcTermOrder, s))
        rddB = rddB.map(lambda rec: recAddPrefix(rec, bcTermOrder, s))
rddA = rddA.map(lambda rec: recAddPrefix(rec, bcTermOrder, s))
rddB = rddB.map(lambda rec: recAddPrefix(rec, bcTermOrder, s))
rddA.collect()
s
s = 0.5
rddA = rddA.map(lambda rec: recAddPrefix(rec, bcTermOrder, s))
rddB = rddB.map(lambda rec: recAddPrefix(rec, bcTermOrder, s))
rddA.collect()
def recAddPrefix(rec, bcTermOrder, s):
    dataset, numId, recId, x, y, terms = rec
    termOrder = bcTermOrder.value
    n = len(termOrder)
    ordered = sorted(terms, key=lambda term: (termOrder[term], term))
    l = int(math.ceil(n * s))
    prefix_len = n - l + 1
    prefix = tuple(ordered[:prefix_len])
    prefixRec = (dataset, numId, recId, x, y, terms, prefix)
    return prefixRec
rddA.collect()
rddA = sc.textFile("in/A.in").map(lineParse)
rddB = sc.textFile("in/B.in").map(lineParse)
rddAPrefix = rddA.map(lambda rec: recAddPrefix(rec, bcTermOrder, s))
rddBPrefix = rddB.map(lambda rec: recAddPrefix(rec, bcTermOrder, s))
rddAPrefix.collect()
rddA.collect()
import math
rddAPrefix = rddA.map(lambda rec: recAddPrefix(rec, bcTermOrder, s))
rddBPrefix = rddB.map(lambda rec: recAddPrefix(rec, bcTermOrder, s))
rddAPrefix.collect()
def recAddPrefix(rec, bcTermOrder, s):
    dataset, numId, recId, x, y, terms = rec
    termOrder = bcTermOrder.value
    orderedTerms = sorted(terms, key=lambda term: (termOrder[term], term))
    n = len(orderedTerms)
    l = int(math.ceil(n * s))
    prefix_len = n - l + 1
    prefix = tuple(orderedTerms[:prefix_len])
    prefixRec = (dataset, numId, recId, x, y, terms, prefix)
    return prefixRec
 
rddAPrefix = rddA.map(lambda rec: recAddPrefix(rec, bcTermOrder, s))
rddBPrefix = rddB.map(lambda rec: recAddPrefix(rec, bcTermOrder, s))
rddAPrefix.collect()
for i in rddAPrefix.collect():
	print(i)
def printloop(item):
	for i in item:
		print(i)
printloop(rddBPrefix.collect())
%logstart
%save
import readline
readline.write_history_file('/home/nadserver/spark-home/ass3/history')
